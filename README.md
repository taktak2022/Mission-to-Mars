# Mission-to-Mars
## Module 10 Web-Scraping

![image](https://user-images.githubusercontent.com/99851509/172485332-71841430-c32c-45fb-8ce9-4737a79f914a.png)

## Overview
This module covers Web-Scraping, a process to extract information from active websites.  The gathered data is then placed in a properly formatted webpage.  The best part is that this webpage can be automated to gather the most current information from those active websites.

We will be assisting Robin, a junior data scientist who moonlights (space pun intended) as a freelance astronomer in her spare time.  She has an idea after spending most of her free time visiting sites about space exploration, focusing on the Mission to Mars as her dream job would be to work for NASA someday.  To further this interest, she decides to create a webpage on her favorite subject which, if polished can shine enough to get the attention of the staff at NASA.

## Tools
* Jupyter Notebook
* VS Code
* Python
* Beautiful Soup: extracts data for analysis
* Splinter: automates the browser to gather data
* MongoDB: a No-SQL database to store the gathered data
* Flask: creates a web framework
* Chrome Driver/Dev Tools
* HTML: framework for our webpage
* CSS
* Bootstrap: styling for our webpage
* Terminal (PythonData): to access our HTTP Url address
* Terminal (PythonData): to run MongoDB
* Terminal (PythonData): to use Jupyter Notebook

## Challenges in Module 10
If you have not had any experience with HTML and creating a webpage, this would be a "baptism by fire" intoduction.  Having got very lost somewhere around Module 10.5.1 - 10.5.3, I turned in what I had knowing it was unfinished.  This module was very confusing.  The list of tools alone was enough to perplex most people, but learning how all these pieces of the puzzle fit together without knowing how the end result was supposed to look like confused some memebers of the class.  I know from my own personal experience that two of the Learning Assistants I reached out to agreed with me that this module could and should have been written describing in finer detail how all these pieces worked in conjuction.  For example, in Module 10.1.1 there are instructions to "open your terminal", but neglects to say which terminal or which environment we should be in to follow this simple instruction.  Did it mean the command line prompt?  Anaconda prompt?  PythonData prompt?  GitBash?  Adding to this confusion was having to have multiple terminals open at the same time when using MongoDB.

When it did finally come all together, the result looks like the following:
![Mission to Mars webpage Mod 10](https://user-images.githubusercontent.com/99851509/172508344-5f08f672-e7fe-4920-8408-ae66f3c5f554.png)

## Summary
In conclusion, I feel as if this module should have come after Module 11 UFOs which explained HTML in finer detail and would take care of explaining the inner workings of HTML in this module.  Another option would be to break this module up with more sections explaining in explicit detail how all the tools listed above related and interacted with each other to create the final picture of this puzzle called "Web-Scraping".
